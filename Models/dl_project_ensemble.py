# -*- coding: utf-8 -*-
"""DL_Proj_Sep_Classes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ttl3H0xXe6ujhuUwA7gugqgaAGYxwUzZ
"""

### Ensemble Models ###


# Commented out IPython magic to ensure Python compatibility.
import sys
import sklearn
import os
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from functools import partial
import PIL
import PIL.Image

import tensorflow as tf
from tensorflow import keras

np.random.seed(42) # note that you must use the same seed to ensure consistentcy in your training/validation/testing
tf.random.set_seed(42)

import pandas as pd
import numpy as np


# each batch in train_ds or validation_ds tensor: 32 image tensors (224 x 224 x 3); 1D tensor with 32 class labels

from sklearn.datasets import load_files 
from keras.utils import np_utils

from keras.preprocessing import image
from tqdm import tqdm # progress bar

data_dir = "/home/cew4pf/dl_project/content/test_dir/"
batch_size = 32;
img_height = 224; 
img_width = 224;

# Training Dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split = 0.2,
    subset = "training",
    seed = 42,
    image_size= (img_height, img_width),
    batch_size = batch_size
)

# Validation Dataset
validation_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    validation_split = 0.2,
    subset = "validation",
    seed = 42,
    image_size = (img_height, img_width),
    batch_size = batch_size
)

keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)


# mobilenet model
mn_model = tf.keras.models.load_model('/home/cew4pf/dl_project/tmp/mn_model')

mn_model._name = 'mn_model'


# inceptionnet model
inc_model = tf.keras.models.load_model('/home/cew4pf/dl_project/tmp/inc_model')

inc_model._name = 'inc_model'


# vgg16 model
vgg_model = tf.keras.models.load_model('/home/cew4pf/dl_project/tmp/vgg_model')

vgg_model._name = 'vgg_model'


# resnet model
res_model = tf.keras.models.load_model('/home/cew4pf/dl_project/tmp/res_model')

res_model._name = 'res_model'


# efficientnet model
eff_model = tf.keras.models.load_model('/home/cew4pf/dl_project/tmp/eff_model')

eff_model._name = 'eff_model'


# ensemble model with all individual model predictions weighted equally
inputs = tf.keras.Input(shape = (224, 224, 3))
outputs = tf.keras.layers.Dense(39, activation = "softmax")(inputs)

models = [mn_model, inc_model, vgg_model, res_model, eff_model]

n_models = len(models)

preds = [model(inputs) for model in models]

outputs = keras.layers.average(preds)

keras.backend.clear_session()

average_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)

base_learning_rate = 0.001

average_ensemble_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = base_learning_rate),
                               loss = "sparse_categorical_crossentropy",
                               metrics = ['accuracy'])

average_ensemble_model.summary()


# test out different combinations of weights for each of the individual models in the ensemble model
from itertools import product

weights = [0.2, 0.4, 0.6, 0.8, 1]

best_accuracy = 0.0
best_weights = None

count = 1

# adapted from How to Develop a Weighted Average Ensemble for Deep Learning Neural Networks by Jason Brownlee, PhD
# https://machinelearningmastery.com/weighted-average-ensemble-for-deep-learning-neural-networks/#:~:text=A+weighted+ensemble+is+an,the+performance+of+the+model
for i in product(weights, repeat = n_models):
    
    # weights
    l1_norm = np.linalg.norm(i, 1)
    if l1_norm == 0.0:
        norm_weights = i
    else:
        norm_weights = i / l1_norm
    
    # ouput layer with given weights
    outputs = keras.layers.average([pred * weight for pred, weight in zip(preds, norm_weights)])
    
    # fit ensemble model with weights
    keras.backend.clear_session()
    ensemble = keras.Model(inputs=inputs, outputs=outputs)
    ensemble.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = base_learning_rate),
                     loss = "sparse_categorical_crossentropy",
                     metrics = ['accuracy'])
    
    # accuracy
    accuracy = ensemble.evaluate(validation_ds)[1]
    
    # get best weights
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_weights = norm_weights
        print('>%s %.3f' % (best_weights, best_accuracy))
    
    # track iterations
    print(count)
    count += 1


# ensemble model with the weights that maximize the validation accuracy
outputs = keras.layers.average([pred * weight for pred, weight in zip(preds, best_weights)])

best_ensemble_model = keras.Model(inputs=inputs, outputs=outputs)

base_learning_rate = 0.001

best_ensemble_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = base_learning_rate),
                            loss = "sparse_categorical_crossentropy",
                            metrics = ['accuracy'])


# evalute validation ds
best_ensemble_model.evaluate(validation_ds)

best_model.save('/home/cew4pf/dl_project/tmp/ensemble_model')
